{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1szvCDdSCLd-0dDydoDxchuLlutHcIq2O",
      "authorship_tag": "ABX9TyOby0fEQqI4uA+mTOQ855+6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dewbeeny/Study/blob/main/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir uploadfoder"
      ],
      "metadata": {
        "id": "yQd9ySLNPlo2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./uploadfoder/ADHD_part1.zip -d ./uploadfoder\n",
        "!unzip ./uploadfoder/ADHD_part2.zip -d ./uploadfoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxIx2OtITX__",
        "outputId": "88a94457-d6ac-4215-8883-53a45de67277"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./uploadfoder/ADHD_part1.zip\n",
            "  inflating: ./uploadfoder/ADHD_part1/v10p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v12p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v14p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v15p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v173.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v18p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v19p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v1p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v20p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v21p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v22p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v24p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v25p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v27p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v28p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v29p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v30p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v31p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v32p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v33p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v34p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v35p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v36p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v37p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v38p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v39p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v3p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v40p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v6p.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part1/v8p.mat  \n",
            "Archive:  ./uploadfoder/ADHD_part2.zip\n",
            "  inflating: ./uploadfoder/ADHD_part2/v177.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v179.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v181.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v183.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v190.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v196.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v198.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v200.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v204.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v206.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v209.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v213.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v215.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v219.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v227.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v231.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v234.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v236.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v238.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v244.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v246.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v250.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v254.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v263.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v265.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v270.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v274.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v279.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v284.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v286.mat  \n",
            "  inflating: ./uploadfoder/ADHD_part2/v288.mat  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip ./uploadfoder/Control_part1.zip -d ./uploadfoder\n",
        "!unzip ./uploadfoder/Control_part2.zip -d ./uploadfoder"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NWNZ-4ATrZB",
        "outputId": "63046069-5d61-495b-9738-60a6be5208cb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ./uploadfoder/Control_part1.zip\n",
            "  inflating: ./uploadfoder/Control_part1/v107.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v108.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v109.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v110.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v111.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v112.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v113.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v114.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v115.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v116.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v41p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v42p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v43p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v44p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v45p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v46p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v47p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v48p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v49p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v50p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v51p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v52p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v53p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v54p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v55p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v56p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v57p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v58p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v59p.mat  \n",
            "  inflating: ./uploadfoder/Control_part1/v60p.mat  \n",
            "Archive:  ./uploadfoder/Control_part2.zip\n",
            "  inflating: ./uploadfoder/Control_part2/v117.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v118.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v120.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v121.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v123.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v125.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v127.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v129.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v131.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v133.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v134.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v138.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v140.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v143.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v147.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v149.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v151.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v297.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v298.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v299.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v300.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v302.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v303.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v304.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v305.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v306.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v307.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v308.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v309.mat  \n",
            "  inflating: ./uploadfoder/Control_part2/v310.mat  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To1x-FGbcaxD",
        "outputId": "72a82717-24ec-4ac3-fc04-a8a8ea42ce5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "pip install scipy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import os\n",
        "\n",
        "# 파일이 저장된 디렉토리 설정\n",
        "data_folder = './drive/MyDrive/uploadfoder/ADHD_part1/'\n",
        "\n",
        "# 디렉토리 내의 모든 .mat 파일 목록 가져오기\n",
        "mat_files = [f for f in os.listdir(data_folder) if f.endswith('.mat')]\n",
        "\n",
        "# 각 파일에서 데이터를 불러오고 내용 확인하기\n",
        "for mat_file in mat_files:\n",
        "    file_path = os.path.join(data_folder, mat_file)\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "\n",
        "    # 데이터 구조 확인 (키 값 출력)\n",
        "    print(f\"File: {mat_file}\")\n",
        "    print(f\"Keys: {data.keys()}\")\n",
        "\n",
        "    # 필요에 따라 실제 데이터 확인\n",
        "    # 여기서는 임의로 'eeg_data'라는 키가 있다고 가정\n",
        "    if 'eeg_data' in data:\n",
        "        eeg_data = data['eeg_data']\n",
        "        print(f\"EEG data shape for {mat_file}: {eeg_data.shape}\")\n",
        "    print('-' * 40)\n",
        "\n",
        "    # 결괏값에서 각 파일별로 저장된 키를 확인하였음.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fRFcUcRmPvQ",
        "outputId": "02d35183-4488-4051-e72f-82645289fbe9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: v31p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v31p'])\n",
            "----------------------------------------\n",
            "File: v18p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v18p'])\n",
            "----------------------------------------\n",
            "File: v12p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v12p'])\n",
            "----------------------------------------\n",
            "File: v27p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v27p'])\n",
            "----------------------------------------\n",
            "File: v6p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v6p'])\n",
            "----------------------------------------\n",
            "File: v24p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v24p'])\n",
            "----------------------------------------\n",
            "File: v29p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v29p'])\n",
            "----------------------------------------\n",
            "File: v32p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v32p'])\n",
            "----------------------------------------\n",
            "File: v14p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v14p'])\n",
            "----------------------------------------\n",
            "File: v21p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v21p'])\n",
            "----------------------------------------\n",
            "File: v30p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v30p'])\n",
            "----------------------------------------\n",
            "File: v40p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v40p'])\n",
            "----------------------------------------\n",
            "File: v35p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v35p'])\n",
            "----------------------------------------\n",
            "File: v22p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v22p'])\n",
            "----------------------------------------\n",
            "File: v19p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v19p'])\n",
            "----------------------------------------\n",
            "File: v10p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v10p'])\n",
            "----------------------------------------\n",
            "File: v173.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v173'])\n",
            "----------------------------------------\n",
            "File: v20p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v20p'])\n",
            "----------------------------------------\n",
            "File: v34p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v34p'])\n",
            "----------------------------------------\n",
            "File: v3p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v3p'])\n",
            "----------------------------------------\n",
            "File: v1p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v1p'])\n",
            "----------------------------------------\n",
            "File: v33p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v33p'])\n",
            "----------------------------------------\n",
            "File: v15p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v15p'])\n",
            "----------------------------------------\n",
            "File: v25p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v25p'])\n",
            "----------------------------------------\n",
            "File: v36p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v36p'])\n",
            "----------------------------------------\n",
            "File: v38p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v38p'])\n",
            "----------------------------------------\n",
            "File: v8p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v8p'])\n",
            "----------------------------------------\n",
            "File: v39p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v39p'])\n",
            "----------------------------------------\n",
            "File: v28p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v28p'])\n",
            "----------------------------------------\n",
            "File: v37p.mat\n",
            "Keys: dict_keys(['__header__', '__version__', '__globals__', 'v37p'])\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "import os\n",
        "\n",
        "# 파일이 저장된 디렉토리 설정\n",
        "data_folder = './drive/MyDrive/uploadfoder/ADHD_part1/'\n",
        "\n",
        "# 디렉토리 내의 모든 .mat 파일 목록 가져오기\n",
        "mat_files = [f for f in os.listdir(data_folder) if f.endswith('.mat')]\n",
        "\n",
        "# 각 파일에서 데이터를 불러오고 내용 확인하기\n",
        "for mat_file in mat_files:\n",
        "    file_path = os.path.join(data_folder, mat_file)\n",
        "    data = scipy.io.loadmat(file_path)\n",
        "\n",
        "    # 각 파일의 주요 데이터 키를 동적으로 가져옴\n",
        "    key = [k for k in data.keys() if not k.startswith('__')][0]  # '__'로 시작하지 않는 실제 데이터 키 가져오기\n",
        "    eeg_data = data[key]\n",
        "\n",
        "    # 데이터의 형태 확인\n",
        "    print(f\"File: {mat_file}\")\n",
        "    print(f\"EEG data shape: {eeg_data.shape}\")\n",
        "    print('-' * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fqqIrCAXEVV",
        "outputId": "76839122-8675-428c-e279-07a27ba012a4"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: v31p.mat\n",
            "EEG data shape: (11679, 19)\n",
            "----------------------------------------\n",
            "File: v18p.mat\n",
            "EEG data shape: (25003, 19)\n",
            "----------------------------------------\n",
            "File: v12p.mat\n",
            "EEG data shape: (17604, 19)\n",
            "----------------------------------------\n",
            "File: v27p.mat\n",
            "EEG data shape: (28880, 19)\n",
            "----------------------------------------\n",
            "File: v6p.mat\n",
            "EEG data shape: (17561, 19)\n",
            "----------------------------------------\n",
            "File: v24p.mat\n",
            "EEG data shape: (16385, 19)\n",
            "----------------------------------------\n",
            "File: v29p.mat\n",
            "EEG data shape: (24193, 19)\n",
            "----------------------------------------\n",
            "File: v32p.mat\n",
            "EEG data shape: (18049, 19)\n",
            "----------------------------------------\n",
            "File: v14p.mat\n",
            "EEG data shape: (17562, 19)\n",
            "----------------------------------------\n",
            "File: v21p.mat\n",
            "EEG data shape: (16574, 19)\n",
            "----------------------------------------\n",
            "File: v30p.mat\n",
            "EEG data shape: (21663, 19)\n",
            "----------------------------------------\n",
            "File: v40p.mat\n",
            "EEG data shape: (20097, 19)\n",
            "----------------------------------------\n",
            "File: v35p.mat\n",
            "EEG data shape: (15305, 19)\n",
            "----------------------------------------\n",
            "File: v22p.mat\n",
            "EEG data shape: (12100, 19)\n",
            "----------------------------------------\n",
            "File: v19p.mat\n",
            "EEG data shape: (23063, 19)\n",
            "----------------------------------------\n",
            "File: v10p.mat\n",
            "EEG data shape: (14304, 19)\n",
            "----------------------------------------\n",
            "File: v173.mat\n",
            "EEG data shape: (24241, 19)\n",
            "----------------------------------------\n",
            "File: v20p.mat\n",
            "EEG data shape: (35328, 19)\n",
            "----------------------------------------\n",
            "File: v34p.mat\n",
            "EEG data shape: (19555, 19)\n",
            "----------------------------------------\n",
            "File: v3p.mat\n",
            "EEG data shape: (33570, 19)\n",
            "----------------------------------------\n",
            "File: v1p.mat\n",
            "EEG data shape: (12258, 19)\n",
            "----------------------------------------\n",
            "File: v33p.mat\n",
            "EEG data shape: (29217, 19)\n",
            "----------------------------------------\n",
            "File: v15p.mat\n",
            "EEG data shape: (43252, 19)\n",
            "----------------------------------------\n",
            "File: v25p.mat\n",
            "EEG data shape: (9894, 19)\n",
            "----------------------------------------\n",
            "File: v36p.mat\n",
            "EEG data shape: (17401, 19)\n",
            "----------------------------------------\n",
            "File: v38p.mat\n",
            "EEG data shape: (24695, 19)\n",
            "----------------------------------------\n",
            "File: v8p.mat\n",
            "EEG data shape: (15776, 19)\n",
            "----------------------------------------\n",
            "File: v39p.mat\n",
            "EEG data shape: (18177, 19)\n",
            "----------------------------------------\n",
            "File: v28p.mat\n",
            "EEG data shape: (27612, 19)\n",
            "----------------------------------------\n",
            "File: v37p.mat\n",
            "EEG data shape: (9286, 19)\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import signal\n",
        "import numpy as np\n",
        "\n",
        "# EEG 데이터를 리샘플링하는 함수 (128Hz -> 512Hz)\n",
        "def resample_eeg_data(eeg_data, original_freq=128, target_freq=512):\n",
        "    num_epochs, num_channels, num_samples = eeg_data.shape\n",
        "    target_samples = int(num_samples * target_freq / original_freq)\n",
        "\n",
        "    resampled_data = np.zeros((num_epochs, num_channels, target_samples))\n",
        "\n",
        "    # 각 에포크와 각 채널별로 리샘플링 진행\n",
        "    for i in range(num_epochs):\n",
        "        for j in range(num_channels):\n",
        "            resampled_data[i, j, :] = signal.resample(eeg_data[i, j, :], target_samples)\n",
        "\n",
        "    return resampled_data\n",
        "\n",
        "# 데이터 리샘플링 및 CNN에 입력할 수 있는 형태로 변환하는 함수\n",
        "def preprocess_and_reshape(data):\n",
        "    # 128Hz로 기록된 데이터를 512Hz로 리샘플링\n",
        "    resampled_data = resample_eeg_data(data)\n",
        "\n",
        "    # CNN 입력을 위해 (epochs, channels, samples, 1) 형태로 변환\n",
        "    reshaped_data = resampled_data[..., np.newaxis]\n",
        "\n",
        "    return reshaped_data\n",
        "\n",
        "# 예시: 파일 하나에서 데이터를 불러와 전처리하기\n",
        "example_file = os.path.join(data_folder, mat_files[0])  # 첫 번째 파일 예시\n",
        "data = scipy.io.loadmat(example_file)\n",
        "\n",
        "if 'eeg_data' in data:\n",
        "    eeg_data = data['eeg_data']  # 실제로 존재하는 키로 변경해야 함\n",
        "    processed_data = preprocess_and_reshape(eeg_data)\n",
        "    print(f\"Processed EEG data shape: {processed_data.shape}\")\n"
      ],
      "metadata": {
        "id": "NTesPg68mYkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df5Ckv9kmqhf",
        "outputId": "dad7e7e2-6f4c-46c2-b87c-c2c6a5ad73bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, AveragePooling1D, Flatten, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# CNN 모델 생성 함수\n",
        "def build_cnn_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    # 첫 번째 합성곱 블록\n",
        "    model.add(Conv1D(filters=16, kernel_size=10, activation='relu', input_shape=input_shape))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling1D(pool_size=2))\n",
        "\n",
        "    # 두 번째 합성곱 블록\n",
        "    model.add(Conv1D(filters=32, kernel_size=4, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling1D(pool_size=2))\n",
        "\n",
        "    # 세 번째 합성곱 블록\n",
        "    model.add(Conv1D(filters=64, kernel_size=4, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(AveragePooling1D(pool_size=2))\n",
        "\n",
        "    # 완전 연결층\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))  # 과적합 방지를 위한 드롭아웃\n",
        "    model.add(Dense(32, activation='relu'))\n",
        "\n",
        "    # 출력층 (이진 분류)\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # 모델 컴파일\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "-0CKRQdJm7c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import scipy.io\n",
        "import os\n",
        "from scipy import signal\n",
        "import numpy as np\n",
        "\n",
        "# EEG 데이터를 리샘플링하는 함수 (128Hz -> 512Hz)\n",
        "def resample_eeg_data(eeg_data, original_freq=128, target_freq=512):\n",
        "    num_epochs, num_channels, num_samples = eeg_data.shape\n",
        "    target_samples = int(num_samples * target_freq / original_freq)\n",
        "\n",
        "    resampled_data = np.zeros((num_epochs, num_channels, target_samples))\n",
        "\n",
        "    # 각 에포크와 각 채널별로 리샘플링 진행\n",
        "    for i in range(num_epochs):\n",
        "        for j in range(num_channels):\n",
        "            resampled_data[i, j, :] = signal.resample(eeg_data[i, j, :], target_samples)\n",
        "\n",
        "    return resampled_data\n",
        "\n",
        "# 데이터 리샘플링 및 CNN에 입력할 수 있는 형태로 변환하는 함수\n",
        "def preprocess_and_reshape(data):\n",
        "    # 128Hz로 기록된 데이터를 512Hz로 리샘플링\n",
        "    resampled_data = resample_eeg_data(data)\n",
        "\n",
        "    # CNN 입력을 위해 (epochs, channels, samples, 1) 형태로 변환\n",
        "    reshaped_data = resampled_data[..., np.newaxis]\n",
        "\n",
        "    return reshaped_data\n",
        "\n",
        "# 예시: 파일 하나에서 데이터를 불러와 전처리하기\n",
        "# 파일이 저장된 디렉토리 설정 # 추가\n",
        "data_folder = './uploadfoder/ADHD_part1/' # 추가\n",
        "# 디렉토리 내의 모든 .mat 파일 목록 가져오기 # 추가\n",
        "mat_files = [f for f in os.listdir(data_folder) if f.endswith('.mat')] # 추가\n",
        "example_file = os.path.join(data_folder, mat_files[0])  # 첫 번째 파일 예시 # 수정\n",
        "data = scipy.io.loadmat(example_file)\n",
        "\n",
        "if 'eeg_data' in data:\n",
        "    eeg_data = data['eeg_data']  # 실제로 존재하는 키로 변경해야 함\n",
        "    processed_data = preprocess_and_reshape(eeg_data) # 이 부분이 실행되어야 processed_data가 정의됩니다.\n",
        "    print(f\"Processed EEG data shape: {processed_data.shape}\")\n",
        "\n",
        "# 예시: 라벨 데이터는 0과 1로 구성된 ADHD 여부를 나타내는 값이라고 가정\n",
        "# labels는 EEG 데이터에 대응하는 ADHD 여부 (0 또는 1)로 구성된 라벨 배열이어야 합니다.\n",
        "# 예를 들어 processed_data가 전처리된 EEG 데이터이고, labels는 이에 대응하는 ADHD 여부입니다.\n",
        "\n",
        "# ----> 여기에 labels 변수를 정의해야 합니다. <---- # 예: labels = np.zeros(processed_data.shape[0])  # 모든 데이터를 0으로 초기화 (예시)\n",
        "\n",
        "# 학습 데이터와 테스트 데이터로 분리 (80% 학습, 20% 테스트)\n",
        "X_train, X_test, y_train, y_test = train_test_split(processed_data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# 모델 입력 형식을 지정 (채널, 샘플, 1)\n",
        "input_shape = X_train.shape[1:]  # (channels, samples, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "CQu1zsUvnSso",
        "outputId": "7f2fcfe6-4f69-496d-d7e5-f992397d9ef2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './uploadfoder/ADHD_part1/'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d453d369d14d>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./uploadfoder/ADHD_part1/'\u001b[0m \u001b[0;31m# 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# 디렉토리 내의 모든 .mat 파일 목록 가져오기 # 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmat_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mexample_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 첫 번째 파일 예시 # 수정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './uploadfoder/ADHD_part1/'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1ojgxpMPnP11"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}